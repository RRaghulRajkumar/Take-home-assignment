{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935ede1c-0c5c-4b32-85a5-8a9e5d7e7fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: timedelta in /opt/anaconda3/lib/python3.11/site-packages (2020.12.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed76c660-874c-44ad-9713-c779e5d8e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Completed. Saved as: Cleaned_Data_QA_Specialist_Final.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"./data/data.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Read all sheets into a dictionary of DataFrames\n",
    "sheets = {sheet_name: xls.parse(sheet_name) for sheet_name in xls.sheet_names}\n",
    "\n",
    "# Function to clean and process the dataset\n",
    "def clean_transaction_data(df):\n",
    "    \"\"\"\n",
    "    Data Cleaning & Validation:\n",
    "    1. Identifies and removes duplicate records\n",
    "    2. Excludes non-spending transactions\n",
    "    3. Assigns start and end dates using a structured approach\n",
    "    4. Handles missing values efficiently\n",
    "    5. Implements error handling to prevent script failures\n",
    "    6. Validates data consistency before finalizing\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Remove Duplicate Transactions\n",
    "        df['duplicate_flag'] = df.duplicated(subset=['date', 'line_amount_usd', 'memo'], keep=False)\n",
    "        \n",
    "        # Prioritize transactions from more reliable integrations (Bill.com > QuickBooks > Brex)\n",
    "        integration_priority = {'bill.com': 3, 'quickbooks': 2, 'brex': 1}\n",
    "        df['integration_score'] = df['integration'].map(integration_priority).fillna(0)\n",
    "        df = df.sort_values(by=['duplicate_flag', 'integration_score'], ascending=[False, False])\n",
    "        df = df.drop_duplicates(subset=['date', 'line_amount_usd', 'memo'], keep='first')\n",
    "        \n",
    "        # Step 2: Exclude Non-Spending Transactions\n",
    "        df['Include/Exclude'] = df['record_type'].apply(lambda x: \"Exclude\" if x == \"journal_entry\" else \"Include\")\n",
    "        \n",
    "        # Step 3: Assign Start and End Dates\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Include/Exclude'] == \"Include\":\n",
    "                memo = str(row['memo']) if pd.notna(row['memo']) else \"\"\n",
    "                \n",
    "                if '-' in memo:\n",
    "                    parts = memo.split('-')\n",
    "                    try:\n",
    "                        df.at[index, 'Start Date'] = pd.to_datetime(parts[0].strip(), errors='coerce')\n",
    "                        df.at[index, 'End Date'] = pd.to_datetime(parts[1].strip(), errors='coerce')\n",
    "                    except Exception:\n",
    "                        df.at[index, 'Start Date'] = row['date']\n",
    "                        df.at[index, 'End Date'] = row['date'] + timedelta(days=30)\n",
    "                else:\n",
    "                    df.at[index, 'Start Date'] = row['date']\n",
    "                    df.at[index, 'End Date'] = row['date'] + timedelta(days=30)\n",
    "            else:\n",
    "                df.at[index, 'Start Date'] = None\n",
    "                df.at[index, 'End Date'] = None\n",
    "        \n",
    "        # Step 4: Fill Missing Memos\n",
    "        df['memo'].fillna(\"No Description Provided\", inplace=True)\n",
    "        \n",
    "        # Step 5: Fill Missing Dates\n",
    "        df['Start Date'].fillna(df['date'], inplace=True)\n",
    "        df['End Date'].fillna(df['Start Date'] + timedelta(days=30), inplace=True)\n",
    "        \n",
    "        # Step 6: Fill Missing 'other_integration_list' Values\n",
    "        if 'other_integration_list' in df.columns:\n",
    "            df['other_integration_list'] = df['other_integration_list'].apply(lambda x: \"No Other Integrations\" if pd.isna(x) or x == \"[]\" else x)\n",
    "        \n",
    "        # Step 7: Add Notes for Justification\n",
    "        df['Notes'] = df.apply(lambda row: \n",
    "            \"Excluded due to duplicate or non-spending record.\" if row['Include/Exclude'] == \"Exclude\" else \n",
    "            \"Included based on valid transaction and inferred service period.\", axis=1)\n",
    "        \n",
    "        return df.drop(columns=['duplicate_flag', 'integration_score'])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data cleaning process: {e}\")\n",
    "        return df\n",
    "\n",
    "# Apply the cleaning function to all sheets\n",
    "cleaned_sheets = {name: clean_transaction_data(df) for name, df in sheets.items()}\n",
    "\n",
    "# Save the cleaned dataset to a new Excel file\n",
    "output_file = \"Cleaned_Data_QA_Specialist_Final.xlsx\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for name, df in cleaned_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=name, index=False)\n",
    "\n",
    "print(f\"Data Cleaning Completed. Saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b96e2a-6286-4631-ab03-ab81f6a12b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
